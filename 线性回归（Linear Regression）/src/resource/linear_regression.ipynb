{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1.线性回归\n",
    "## 1.1 线性模型(Linear Model)\n",
    "给定数据集$D=\\left \\{ \\left (x_{1}\\mathbf{},y_{1} \\right ),\\left ( x_{2},y_{2} \\right ),\\left (x_{3},y_{3} \\right ),...,\\left (x_{m},y_{m} \\right ) \\right \\}$，其中$x_{i}$有$d$个特征，表示为$x_{i}=\\left ( x_{i1}; x_{i1};...;x_{id}\\right )$,线性回归模型是通过$d$个特征的线性组合对$y$值进行拟合，即$f(x_{i})=w^{T}x_{i}+b$,使得$f(x_{i})\\simeq y_{i}$\n",
    "## 1.2 损失函数(Loss Function):Square Loss\n",
    "\n",
    " - 定义：$L(f(x),y)=(f(x)-y)^2$\n",
    "- 为什么使用square loss作为损失函数：\n",
    "\n",
    "> (1)记误差$\\varepsilon=y_{i}-\\hat{y_{i}}$,假设误差独立同分布，根据中心极限定理，$\\varepsilon\\sim(\\mu,\\sigma^2)$,得到$f(\\varepsilon)=\\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(\\varepsilon-\\mu)^2}{2\\sigma^2})$,求$\\mu$和$\\sigma^2$的极大似然估计，\n",
    "$L(\\mu,\\sigma^2)=\\prod_{i=1}^{m}\\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(\\varepsilon-\\mu)^2}{2\\sigma^2})$,等式两边取对数，得到，\n",
    "$logL(\\mu,\\sigma^2)=-\\frac{m}{2}log2\\pi-\\frac{m}{2}log\\sigma^2-\\frac{(\\varepsilon-\\mu)^2}{2\\sigma^2}$,对$\\mu$和$\\sigma^2$求偏导，得到\n",
    "$\\frac{\\partial{L}}{\\partial{\\mu}}=\\frac{1}{\\sigma^2}(\\varepsilon-\\mu)$,\n",
    "$\\frac{\\partial{L}}{\\partial{\\sigma^2}}=\\frac{m}{2\\sigma^2}+\\frac{(\\varepsilon-\\mu)^2}{2\\sigma^4}$,\n",
    "令$\\frac{\\partial{L}}{\\partial{\\mu}}=\\frac{\\partial{L}}{\\partial{\\sigma^2}}=0$，得到：$\\mu=\\frac{1}{m}\\sum_{i=1}^{m}\\varepsilon_{i}=\\frac{1}{m}\\sum_{i=1}^{m}(y_{i}-\\hat{y_{i}})$趋近于0或越小越好 ,$\\sigma^2=\\frac{1}{m}\\sum_{i=1}^{m}(\\varepsilon_{i}-\\mu)^2=\\frac{1}{m}\\sum_{i=1}^{m}(y_{i}-\\hat{y_{i}}-\\mu)^2\\approx\\frac{1}{m}\\sum_{i=1}^{m}(y_{i}-\\hat{y_{i}})^2$趋近于0或越小越好,与最前面构建的平方形式损失函数本质上是等价的。\n",
    "\n",
    ">(2)误差的平方形式是正数。这样正的误差和负的误差不会相互抵消。\n",
    "\n",
    ">(3)与采用绝对值形式的比较：平方形式对大误差的惩罚大于小误差；平方形式对数学运算也更友好，绝对值形式在求导使需要分段求导。\n",
    "\n",
    "- 定义代价函数(cost function):\n",
    "$J(w,b)=\\frac{1}{2m}\\sum_{i=1}^{m}(f(x_{i})-y_{i})^2$\n",
    "\n",
    "## 1.3 求解\n",
    "\n",
    "引入向量形式\n",
    "$\\theta=(w,b),X=\\begin{bmatrix} x_{11}  & x_{12} & \\cdots  &  x_{1d}&1 \\\\ \n",
    "x_{21}  & x_{22} & \\cdots &  x_{2d}&1 \\\\ \n",
    "\\vdots  &\\vdots & \\ddots  &  \\vdots&\\vdots\\\\ \n",
    " x_{m1}  & x_{m2} & \\cdots  &  x_{md}&1 \\end{bmatrix}=\\begin{bmatrix} \n",
    "x_{1}^{T}&1  \\\\ \n",
    "x_{2}^{T}&1 \\\\ \n",
    "\\vdots  &\\vdots \\\\ \n",
    "x_{m}^{T} & 1 \\end{bmatrix}$,\n",
    "则$J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^2=\\frac{1}{2}(X\\theta-y)^T(X\\theta-y)$\n",
    "\n",
    "### 1.3.1 最小二乘法(Least Square Method)\n",
    "\n",
    "在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。\n",
    ">求梯度\n",
    ">$\\nabla_{\\theta}J(\\theta)$\n",
    "\n",
    ">$=\\nabla_{\\theta}\\left(\\frac{1}{2}(\\theta^TX^T-y^T)(X\\theta-y)\\right)$\n",
    "\n",
    ">$=\\nabla_{\\theta}\\left(\\frac{1}{2}(\\theta^TX^TX\\theta-\\theta^TX^Ty-y^TX\\theta+y^Ty)\\right)$\n",
    "\n",
    ">$=\\frac{1}{2}\\left(2X^TX\\theta-X^Ty-(y^TX)^T\\right)$\n",
    "\n",
    ">$=X^TX\\theta-X^Ty$\n",
    "\n",
    ">令上式等于0，求驻点，则有$\\theta=(X^TX)^{-1}X^Ty$,\n",
    "\n",
    ">则令$\\hat{x_{i}}=(x_{i};1)$线性回归模型为$f(X)=\\hat{x_{i}}^T(X^TX)^{-1}X^Ty$\n",
    "\n",
    "### 1.3.2\t梯度下降法(Gradient Descent)\n",
    "\n",
    ">1.初始化$\\theta$\n",
    ">2.沿着负梯度方向迭代，得到$\\theta=\\theta-\\alpha\\cdot\\frac{\\partial J}{\\partial \\theta}$，使得$J(\\theta)$更小，$\\alpha$为学习率(learning rate)(也称为步长)，\n",
    ">偏导计算\n",
    "$\\frac{\\partial J}{\\partial \\theta_{j}}=(h_{\\theta}(x)-y)\\cdot\\frac{\\partial }{\\partial \\theta_j}(\\sum_{i=1}^{m}\\theta_{i}x_{i}-y)=(h_{\\theta}(x)-y)x_j$\n",
    ">3.直至下降到无法下降时，停止计算\n",
    "\n",
    "批量梯度下降法（Batch Gradient Descent）:在更新参数时使用所有的样本来进行更新\n",
    "随机梯度下降法（Stochastic Gradient Descent）:仅选取一个样本来求梯度\n",
    "小批量梯度下降法（Mini-batch Gradient Descent）:对于m个样本，采用x个样本进行迭代\n",
    "\n",
    "## 1.4 正则化(Regularization)\n",
    "\n",
    "现实往往其变量数目超过样本数，$X^TX$不满秩，导致存在多个$\\hat{\\theta}$,使得$J(\\theta)$最小化。此时，选择哪个解作为输出，由学习算法的归纳偏好决定，常见的做法是引入正则化项，通过减少参数$\\theta$的值，使得模型变得简单，避免过拟合。\n",
    "正则化线性回归(regularized linear regression)的代价函数（cost function）为：\n",
    "\n",
    " - L1-norm(LASSO):\n",
    " 假设参数的先验分布是Laplace分布，\n",
    " $J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^2+\\lambda\\sum_{i=1}^{m}|\\theta_j|$\n",
    "  L1范数可以实现稀疏化，而本质上对$\\theta$进行稀疏约束，即希望$\\theta$非零分量尽可能少，常用L0范数，但L0范数不连续，很难优化求解（NP问题）。而在求解向量足够稀疏的情况下，L0范数优化问题等价于L1范数优化问题，即各向量分量绝对值之和。\n",
    " L1的优势在于通过稀疏化可用于特征选择，但也因此导致局部特征没有完全体现出来。\n",
    " -  L2-norm(Ridge):\n",
    " 假设参数的先验分布是Gaussian 分布，\n",
    " $J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^2+\\lambda\\sum_{i=1}^{m}\\theta_j^2$\n",
    "  为了解决L1没有保留局部特征的问题，引入L2范数替代L1范数。L2范数使得$\\theta$趋向于0，$\\theta$越小使得模型的复杂度降低。\n",
    " - Elastic Net:\n",
    " $J(\\theta)=\\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^2+\\lambda\\left(\\rho\\cdot\\sum_{i=1}^{m}|\\theta_j|+(1-\\rho)\\sum_{i=1}^{m}\\theta_j^2\\right)$\n",
    "其中，L1正则项产生稀疏模型，L2正则项消除L1正则项中选择变量个数的限制（即稀疏性）。\n",
    "Elastic Net在高度相关变量的情况下，会产生群体效应；选择变量的数目没有限制；可以承受双重收缩。\n",
    "\n",
    "# 2.广义线性回归(Generalized linear model，GLM)\n",
    "\n",
    "## 2.1 定义\n",
    "\n",
    "### 2.1.1 指数族分布\n",
    "\n",
    "- 给定随机变量$Y$，观测值为$y$,服从指数型分布，即有概率分布函数：\n",
    "$p(y;η)=b(y)exp(η^TT(y)−a(η))$,其中$η$为自然参数(bature parameter)，$T(y)$是充分统计量(sufficient statistic)。\n",
    "- 常见的指数族分布：伯努利分布、高斯分布、多项式分布、泊松分布、指数分布、伽马分布、贝塔分布、狄利克雷分布、维希特分布……\n",
    "\n",
    "### 2.1.2 广义线性模型\n",
    "GLM必须满足以下3个假设：\n",
    " - $y|x,\\theta\\sim ExponentialFamily(η)$\n",
    " - 预测的值$h_{\\theta}(x)=E[T(y)|x]$,通常$T(y)=y$\n",
    " 假设$E(Y)=\\mu$,且$\\mu$与$\\eta$由如下关系:$g(\\mu)=\\eta$,则$\\mu=g^{-1}(\\eta)$,$g(\\mu)$称之为联系函数(link function)。\n",
    " - $η=\\theta^Tx$\n",
    "\n",
    "## 2.2 逻辑回归(Logistic Regression)\n",
    "\n",
    "### 2.2.1 定义\n",
    "\n",
    " - $y\\sim B(1,\\varphi)$\n",
    " $F(y)=\\varphi^y(1-\\varphi)^{(1-y)}$\n",
    " $=exp\\left(ylog\\varphi+(1-y)log(1-\\varphi)\\right)$\n",
    " $=exp\\left(ylog\\frac{\\varphi}{1-\\varphi}+log(1-\\varphi)\\right)$\n",
    " 由指数族分布可知，$b(y)=1,η=log\\frac{\\varphi}{1-\\varphi}$(称为logit),$T(y)=y,a(η)=-log(1-\\varphi)$\n",
    " 得到，$\\varphi=\\frac{1}{1+e^{-η}}$(sigmoid函数)\n",
    " - $h_{\\theta}(x)=E[T(y)|x]=\\varphi$\n",
    " - $η=\\theta^Tx$\n",
    "由此得到$h_{\\theta}(x)=\\frac{1}{1+e^{\\theta^Tx}}$\n",
    "\n",
    "### 2.2.2 损失函数(Loss Function)\n",
    "\n",
    "loss function：对数损失, 即对数似然损失(Log-likelihood Loss), 也称逻辑斯谛回归损失(Logistic Loss)或交叉熵损失(cross-entropy Loss)\n",
    ">$L(\\theta)=\\prod_{i=1}^{m}P(y|x;\\theta)$\n",
    "=$\\prod_{i=1}^{m}h_{\\theta}(x_i)^{y_i}(1-h_{\\theta}(x_i))^{(1-y_i)}$\n",
    "$logL(\\theta)$\n",
    "$=\\sum_{i=1}^{m}log(h_{\\theta}(x_i)^{y_i}(1-h_{\\theta}(x_i))^{(1-y_i)})$\n",
    "$=\\sum_{i=1}^{m}[y_ilogh_{\\theta}(x_i)+(1-y_i)log(1-h_{\\theta}(x_i))]$\n",
    "\n",
    "cost function通过最小化负的似然函数得到：\n",
    "$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}[y_ilogh_{\\theta}(x_i)+(1-y_i)log(1-h_{\\theta}(x_i))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
