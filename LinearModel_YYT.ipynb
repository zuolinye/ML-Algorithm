{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义：<br>\n",
    "给定由d个属性描述的示例$x=(x_1;x_2;...;x_d)$,其中x_i是x在第i个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即$f(x)=w_1x_1+w_2x_2+...+w_dx_d+b$，一般用向量形式写成$f(x)=w^Tx+b$，其中$w=(w_1,...,w_d)$，w和b学得之后，模型就得以确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：<br>\n",
    "（1）形式简单，易于建模；<br>\n",
    "（2）w直观的表达了各属性在预测中的重要性，因此线性模型有很好的可解释性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义：<br>\n",
    "给定数据集$D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$，其中$x_i=(x_{i1},x_{i2},...,x_{id})$，$y_i\\in R$。‘线性回归‘试图学得一个线性模型以尽可能准确地预测实值输出标记，即$f(x_i)=wx_i+b$，使得$f(x_i)\\backsimeq y_i$。<br>\n",
    "线性回归又包括：\n",
    "简单线性回归:（数据集D中样本是有1个属性所描述，类似于一元线性函数$y=wx+b$）；<br>\n",
    "多元线性回归:（数据集D中样本由d个属性所描述，$y=W^TX+b$）；<br>\n",
    "对数线性回归: $\\ln y=w^T+b$（试图让$e^{w^Tx+b}$逼近y）---形式上仍是线性回归，实质上是在求取输入空间到输出空间的非线性映射；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 广义线性模型\n",
    "形如$y=g^{-1}(w^T+b)$，g(.)称为联系函数，是单调可微函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上多是使用线性模型进行回归学习，倘若所做的是分类任务，则只需找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。对于二分类任务，常使用对数几率函数（sigmoid函数），这也就是逻辑回归的来源。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型的预测值逼近y的衍生物可得到输入空间到输出空间的非线性函数映射，比如$\\ln y=w^Tx+b$，就是使用的线性模型的预测值来逼近y的ln值。<br>\n",
    "将线性回归模型 $w^T+b$作为sigmoid函数中的x带入可得：$ y=\\frac{1}{1+e^{-（w^T+b）}} $。<br>\n",
    "也即$$\\ln {\\frac{y}{1-y}}=w^T+b$$<br>\n",
    "将y视为样本x作为正例的可能性，则1-y是其反例可能性，两者比值$\\frac{y}{1-y}$称为‘几率’，反映了x作为正例的可能性。上式等式左边就是对数几率，这个式子也表示了用线性回归模型的预测结果去逼近真实标记的对数几率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：<br>\n",
    "（1）直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了分布不准确所带来的问题；<br>\n",
    "（2）不仅预测出“类别”，而是可得到近似概率预测，对许多需要利用概率辅助决策的任务很有用；<br>\n",
    "（3）对率函数是任意阶可导的凸函数，有很好的数学性质，许多数值优化算法可直接用于求取最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`多分类学习方法可由二分类学习方法推广得到。(有待实验)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.线性回归与逻辑回归的联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论线性回归还是逻辑回归，二者都是线性模型，都是基于线性模型进行实值预测。<br>\n",
    "不同的点在于：<br>\n",
    "线性回归用于回归任务，输入是连续型的数值变量，逻辑回归用于分类任务，输入是离散型的数值变量；<br>\n",
    "线性回归的参数计算方式使用最小二乘法，逻辑回归的参数计算方式采用最大似然估计；<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型的推导   \n",
    "求解w和b，可以使用求导的方法，也可以使用梯度下降的方法。 以下采用求导的思路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)简单线性回归    \n",
    "线性回归试图学得$f(x_i)=wx_i+b$，使得$f(x_i)\\backsimeq y_i$，衡量f(x)与y之间的差别最常用的是均方误差，因此可试图让均方误差最小化（也即欧几里得距离最小）。<br>    \n",
    "$$  \n",
    "\\begin{align}\n",
    "(w^*,b^*) &=\\mathop{\\arg\\min}_{(w,b)}\\sum_{i=1}^m(f(x_i)-y_i)^2\\\\\n",
    "&=\\mathop{\\arg\\min}_{(w,b)}\\sum_{i=1}^m(y_i-wx_i-b)^2\n",
    "\\end{align}\n",
    "$$   \n",
    "基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。线性回归模型就是使用最小二乘“参数估计”进行求解w,b，以使$E_{(w,b)}=\\sum_{i=1}^m(y_i-wx_i-b)^2$最小化。\n",
    "对最小化式进行求导，即可得下式：<br>\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E_(w,b)}{\\partial w}=&2\\left(w\\sum_{i=1}^mx_i^2-\\sum_{i=1}^m(y_i-b)x_i\\right) \\\\\n",
    "\\frac{\\partial E_(w,b)}{\\partial b}=&2\\left(mb=\\sum_{i=1}^m(y_i-wx_i)\\right)\n",
    "\\end{align}$$\n",
    "另上式求导后为0可得到w和b的最优解的闭式解。如下所示：<br>\n",
    "$$\\begin{align}\n",
    "w=&\\frac{\\sum_{i=1}^m y_i(x_i-\\overline{x})}{\\sum_{i=1}^m x_i^2-\\frac{1}{m}\\left(\\sum_{i=1}^m x_i^2\\right)} \\ \\ \\ \\ (\\overline{x}=\\frac{1}{m}\\left(\\sum_{i=1}^mx_i)\\right)^2\\\\\n",
    "b=&\\frac{1}{m}\\sum_{i=1}^m\\left(y_i-wx_i\\right)\n",
    "\\end{align}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）多元线性回归  \n",
    "$f(x_i)=w^Tx_i+b$，使得$f(x_i)\\backsimeq y_i$ 。（$x_i$由d个属性描述）  \n",
    "x是一个m*d维的矩阵，表示m个样本的d个属性，将偏置b纳入w向量中，得到$\\hat{w}=(w;b)$ ，进而x中增加一列全1，对应w中的b。  \n",
    "$$\n",
    "X=\\left(\n",
    "\\begin{matrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d} & 1\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d} & 1\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "x_{m1} & x_{m2} & \\cdots & x_{md} & 1\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "=\\left(\n",
    "\\begin{matrix}\n",
    "x_1^T & 1 \\\\\n",
    "x_2^T & 1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_m^T & 1\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$ \n",
    "进而有如下：  \n",
    "\n",
    "$$\n",
    "\\hat{w}^*=\\mathop{\\arg\\min}_{\\hat{w}}(y-x\\hat{w})^T(y-x\\hat{w})\n",
    "$$      \n",
    "\n",
    "令      \n",
    "$$E_{\\hat{w}}=(y-x\\hat{w})^T(y-x\\hat(w))$$对$\\hat{w}$求导得到$\\frac{\\partial   E_{\\hat{w}}}{\\partial {\\hat{w}}}=2x^T(x\\hat{w}-y)$，令其为0，可得$\\hat{w}$最优解的闭式解。   \n",
    "得到如下：$x^Tx\\hat{w}=x^Ty$，需要分情况讨论解。  \n",
    "（1）$x^Tx$为满秩矩阵或者正定矩阵时，$\\hat{w}^*=(x^Tx)^{-1}x^Ty$，最后学得的线性回归模型为$f(\\hat{x_i})=\\hat{x_i}^T(x^Tx)^{-1}x^Ty$  \n",
    "（2）$x^Tx$不满秩时，可解出多个$\\hat{w}$，均可以使均方误差最小化，选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法会引入正则化项`（这一点有待深究）`。\n",
    "\n",
    "note:正定矩阵：若M是n阶方阵，对任何非零向量$z$，有$z^TMz>0$，则M称为正定矩阵。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法：\n",
    "批量梯度下降，随机梯度下降，小批量梯度下降（有待进一步的细写）\n",
    "[可参考爖的笔记](https://note.youdao.com/share/?id=981825c617d47c10f4e0c373e8b7bfff&type=note#/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LinearRegression中的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linearRegression(fit_intercept=True,nomalize=False,copy_X=True,n_jobs=None)  \n",
    "参数：  \n",
    ">fit_intercept:  boolean类型，默认为True,是否计算截距，若设置成False,不会计算截距，此时过原点（数据进行了中心化）\n",
    ">nomalize:  boolean类型，默认为false，若fit_intercept设置为False，此参数忽略。此参数为True时，x减去均值并除以l2norm;为False，x在fit之前使用sklearn.processing.standardScaler进行标准化。  \n",
    ">copy_X:  为True时，会被复制，否则会被覆写。  \n",
    ">n_jobs:  默认为1，计算时CPU的核数。  \n",
    "\n",
    "属性：  \n",
    ">coef_:输入模型的系数（权重）  \n",
    ">intercept_:截距  \n",
    "\n",
    "方法：\n",
    ">fit(x,y[,sample_weight=None]):训练模型  \n",
    ">get_params(deep_True):返回对regressor的设置值  \n",
    ">predict(x):预测  \n",
    ">score(x,y,sample_weight=None):评估  \n",
    ">set_params(**params)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型的代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用sklearn实现线性回归、逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归模型的评价指标：MSE,RMSE,MAE,R-squared        \n",
    "[Heitao的GitHub](https://github.com/Heitao5200/Heitao5200_MachineLearning/blob/master/LR/LinearRegression.md)  \n",
    "[方naoke的csdn](https://blog.csdn.net/skullFang/article/details/79107127)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  线性回归，Ridge回归（L2范式），Lasso回归（L1范式）三者的联系\n",
    "[有待考究-问题来源此博客](https://blog.csdn.net/q370835062/article/details/82943365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：周志华《机器学习》\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\" \" target=\"_blank\">< img src=\"http://latex.codecogs.com/gif.latex?\\hat&space;y^{(i)}&space;=&space;\\theta&space;_0&space;x_0^{(i)}&plus;&space;\\theta_1&space;x_1^{(i)}&plus;&space;\\theta_2&space;x_2^{(i)}&plus;\\cdots&space;&plus;&space;\\theta_n&space;x_n^{(i)}\" title=\"\\hat y^{(i)} = \\theta _0 x_0^{(i)}+ \\theta_1 x_1^{(i)}+ \\theta_2 x_2^{(i)}+\\cdots + \\theta_n x_n^{(i)}\" /></a >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
